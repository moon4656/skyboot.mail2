"""
ì˜¤í”„ë¼ì¸ ê¸°ëŠ¥ ì„œë¹„ìŠ¤

SkyBoot Mail SaaS í”„ë¡œì íŠ¸ì˜ ì˜¤í”„ë¼ì¸ ê¸°ëŠ¥ì„ ìœ„í•œ ì„œë¹„ìŠ¤ ë¡œì§ì…ë‹ˆë‹¤.
"""

import json
import logging
import asyncio
import gzip
import base64
import hashlib
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
from pathlib import Path
import uuid

from sqlalchemy.orm import Session
from sqlalchemy import and_, or_, func, desc
from fastapi import HTTPException, status, BackgroundTasks
from fastapi.responses import JSONResponse
import redis
from cryptography.fernet import Fernet

from app.schemas.offline_schema import (
    OfflineActionData, SyncTask, CacheItem, ConflictItem, OfflineSettings,
    OfflineActionRequest, OfflineActionResponse,
    SyncRequest, SyncResponse, SyncStatusResponse,
    CacheStatusResponse, OfflineSettingsRequest, OfflineSettingsResponse,
    ConflictListResponse, ConflictResolutionRequest, ConflictResolutionResponse,
    OfflineStatsResponse, NetworkStatusResponse,
    SyncStatus, SyncDirection, OfflineAction, ConflictResolution,
    CacheStrategy, DataType,
    OfflineError, SyncError, CacheError, ConflictError
)
from app.model.organization_model import Organization
from app.model.user_model import User
from app.config import settings

logger = logging.getLogger(__name__)

# Redis ì—°ê²°
redis_client = redis.Redis(
    host=settings.REDIS_HOST,
    port=settings.REDIS_PORT,
    db=settings.REDIS_DB,
    decode_responses=True
)

# ì•”í˜¸í™” í‚¤ (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì•ˆì „í•œ í‚¤ ê´€ë¦¬ í•„ìš”)
ENCRYPTION_KEY = Fernet.generate_key()
cipher_suite = Fernet(ENCRYPTION_KEY)


class OfflineService:
    """ì˜¤í”„ë¼ì¸ ê¸°ëŠ¥ ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self, db: Session):
        self.db = db
        self.cache_ttl = 86400  # 24ì‹œê°„
        self.action_queue_key = "offline:actions:{user_id}"
        self.sync_task_key = "offline:sync_task:{task_id}"
        self.cache_key = "offline:cache:{user_id}:{cache_key}"
        self.settings_key = "offline:settings:{user_id}"
        self.conflict_key = "offline:conflicts:{user_id}"
        
    async def queue_offline_action(
        self,
        user_id: int,
        organization_id: int,
        action_request: OfflineActionRequest
    ) -> OfflineActionResponse:
        """
        ì˜¤í”„ë¼ì¸ ì•¡ì…˜ì„ íì— ì¶”ê°€í•©ë‹ˆë‹¤.
        
        Args:
            user_id: ì‚¬ìš©ì ID
            organization_id: ì¡°ì§ ID
            action_request: ì˜¤í”„ë¼ì¸ ì•¡ì…˜ ìš”ì²­
            
        Returns:
            ì˜¤í”„ë¼ì¸ ì•¡ì…˜ ì‘ë‹µ
        """
        try:
            logger.info(f"ğŸ“± ì˜¤í”„ë¼ì¸ ì•¡ì…˜ í ì¶”ê°€ ì‹œì‘ - ì‚¬ìš©ì: {user_id}, ì•¡ì…˜: {action_request.action_type}")
            
            # ì•¡ì…˜ ID ìƒì„±
            action_id = str(uuid.uuid4())
            
            # ì•¡ì…˜ ë°ì´í„° ìƒì„±
            action_data = OfflineActionData(
                action_id=action_id,
                user_id=user_id,
                organization_id=organization_id,
                action_type=action_request.action_type,
                target_id=action_request.target_id,
                data=action_request.data,
                timestamp=action_request.timestamp or datetime.utcnow(),
                sync_status=SyncStatus.PENDING
            )
            
            # Redis íì— ì¶”ê°€
            queue_key = self.action_queue_key.format(user_id=user_id)
            redis_client.lpush(queue_key, action_data.json())
            
            # í ë§Œë£Œ ì‹œê°„ ì„¤ì • (7ì¼)
            redis_client.expire(queue_key, 604800)
            
            logger.info(f"âœ… ì˜¤í”„ë¼ì¸ ì•¡ì…˜ í ì¶”ê°€ ì™„ë£Œ - ì•¡ì…˜ID: {action_id}")
            
            return OfflineActionResponse(
                action_id=action_id,
                success=True,
                message="ì˜¤í”„ë¼ì¸ ì•¡ì…˜ì´ íì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.",
                sync_status=SyncStatus.PENDING,
                created_at=action_data.created_at
            )
            
        except Exception as e:
            logger.error(f"âŒ ì˜¤í”„ë¼ì¸ ì•¡ì…˜ í ì¶”ê°€ ì‹¤íŒ¨ - ì‚¬ìš©ì: {user_id}, ì˜¤ë¥˜: {str(e)}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="ì˜¤í”„ë¼ì¸ ì•¡ì…˜ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            )
    
    async def start_sync(
        self,
        user_id: int,
        organization_id: int,
        sync_request: SyncRequest,
        background_tasks: BackgroundTasks
    ) -> SyncResponse:
        """
        ë™ê¸°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.
        
        Args:
            user_id: ì‚¬ìš©ì ID
            organization_id: ì¡°ì§ ID
            sync_request: ë™ê¸°í™” ìš”ì²­
            background_tasks: ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…
            
        Returns:
            ë™ê¸°í™” ì‘ë‹µ
        """
        try:
            logger.info(f"ğŸ”„ ë™ê¸°í™” ì‹œì‘ - ì‚¬ìš©ì: {user_id}, ë°ì´í„°íƒ€ì…: {sync_request.data_types}")
            
            # ì‘ì—… ID ìƒì„±
            task_id = str(uuid.uuid4())
            
            # ë™ê¸°í™” ì‘ì—… ìƒì„±
            sync_task = SyncTask(
                task_id=task_id,
                user_id=user_id,
                organization_id=organization_id,
                data_type=sync_request.data_types[0] if sync_request.data_types else DataType.MAIL,
                direction=sync_request.direction,
                status=SyncStatus.PENDING,
                start_time=datetime.utcnow()
            )
            
            # Redisì— ì‘ì—… ì €ì¥
            task_key = self.sync_task_key.format(task_id=task_id)
            redis_client.setex(task_key, 3600, sync_task.json())  # 1ì‹œê°„ TTL
            
            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë™ê¸°í™” ì‹¤í–‰
            background_tasks.add_task(
                self._execute_sync,
                task_id,
                user_id,
                organization_id,
                sync_request
            )
            
            logger.info(f"âœ… ë™ê¸°í™” ì‹œì‘ ì™„ë£Œ - ì‘ì—…ID: {task_id}")
            
            return SyncResponse(
                task_id=task_id,
                status=SyncStatus.PENDING,
                message="ë™ê¸°í™”ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
                estimated_duration=300,  # 5ë¶„ ì˜ˆìƒ
                started_at=sync_task.start_time
            )
            
        except Exception as e:
            logger.error(f"âŒ ë™ê¸°í™” ì‹œì‘ ì‹¤íŒ¨ - ì‚¬ìš©ì: {user_id}, ì˜¤ë¥˜: {str(e)}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="ë™ê¸°í™” ì‹œì‘ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            )
    
    async def get_sync_status(self, task_id: str) -> SyncStatusResponse:
        """
        ë™ê¸°í™” ìƒíƒœë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
        
        Args:
            task_id: ì‘ì—… ID
            
        Returns:
            ë™ê¸°í™” ìƒíƒœ ì‘ë‹µ
        """
        try:
            logger.info(f"ğŸ” ë™ê¸°í™” ìƒíƒœ ì¡°íšŒ - ì‘ì—…ID: {task_id}")
            
            # Redisì—ì„œ ì‘ì—… ì¡°íšŒ
            task_key = self.sync_task_key.format(task_id=task_id)
            task_data = redis_client.get(task_key)
            
            if not task_data:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail="ë™ê¸°í™” ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )
            
            sync_task = SyncTask.parse_raw(task_data)
            
            # ì˜ˆìƒ ë‚¨ì€ ì‹œê°„ ê³„ì‚°
            estimated_remaining = None
            if sync_task.status == SyncStatus.IN_PROGRESS and sync_task.progress > 0:
                elapsed = (datetime.utcnow() - sync_task.start_time).total_seconds()
                estimated_total = elapsed / (sync_task.progress / 100)
                estimated_remaining = max(0, int(estimated_total - elapsed))
            
            logger.info(f"âœ… ë™ê¸°í™” ìƒíƒœ ì¡°íšŒ ì™„ë£Œ - ì‘ì—…ID: {task_id}, ìƒíƒœ: {sync_task.status}")
            
            return SyncStatusResponse(
                task_id=task_id,
                status=sync_task.status,
                progress=sync_task.progress,
                total_items=sync_task.total_items,
                processed_items=sync_task.processed_items,
                failed_items=sync_task.failed_items,
                estimated_remaining=estimated_remaining,
                error_message=sync_task.error_message,
                start_time=sync_task.start_time,
                end_time=sync_task.end_time
            )
            
        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"âŒ ë™ê¸°í™” ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨ - ì‘ì—…ID: {task_id}, ì˜¤ë¥˜: {str(e)}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="ë™ê¸°í™” ìƒíƒœ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            )
    
    async def get_cache_status(self, user_id: int) -> CacheStatusResponse:
        """
        ìºì‹œ ìƒíƒœë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
        
        Args:
            user_id: ì‚¬ìš©ì ID
            
        Returns:
            ìºì‹œ ìƒíƒœ ì‘ë‹µ
        """
        try:
            logger.info(f"ğŸ“Š ìºì‹œ ìƒíƒœ ì¡°íšŒ ì‹œì‘ - ì‚¬ìš©ì: {user_id}")
            
            # ì‚¬ìš©ì ìºì‹œ í‚¤ íŒ¨í„´
            cache_pattern = f"offline:cache:{user_id}:*"
            cache_keys = redis_client.keys(cache_pattern)
            
            total_items = len(cache_keys)
            total_size_bytes = 0
            data_type_breakdown = {}
            oldest_date = None
            newest_date = None
            
            for key in cache_keys:
                try:
                    cache_data = redis_client.get(key)
                    if cache_data:
                        cache_item = CacheItem.parse_raw(cache_data)
                        total_size_bytes += cache_item.size_bytes
                        
                        # ë°ì´í„° íƒ€ì…ë³„ ë¶„ë¥˜
                        data_type = cache_item.data_type.value
                        data_type_breakdown[data_type] = data_type_breakdown.get(data_type, 0) + 1
                        
                        # ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
                        if oldest_date is None or cache_item.created_at < oldest_date:
                            oldest_date = cache_item.created_at
                        if newest_date is None or cache_item.created_at > newest_date:
                            newest_date = cache_item.created_at
                            
                except Exception as e:
                    logger.warning(f"ìºì‹œ í•­ëª© íŒŒì‹± ì‹¤íŒ¨ - í‚¤: {key}, ì˜¤ë¥˜: {str(e)}")
                    continue
            
            # ì‚¬ìš©ì ì„¤ì • ì¡°íšŒ
            settings = await self.get_offline_settings(user_id, 1)  # ì„ì‹œ ì¡°ì§ ID
            max_size_bytes = settings.max_cache_size * 1024 * 1024  # MB to bytes
            
            total_size_mb = total_size_bytes / (1024 * 1024)
            used_size_mb = total_size_mb
            available_size_mb = max(0, (max_size_bytes / (1024 * 1024)) - total_size_mb)
            
            # ìºì‹œ ì ì¤‘ë¥  ê³„ì‚° (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë³„ë„ ë©”íŠ¸ë¦­ í•„ìš”)
            cache_hit_rate = 85.0  # ìƒ˜í”Œ ê°’
            
            logger.info(f"âœ… ìºì‹œ ìƒíƒœ ì¡°íšŒ ì™„ë£Œ - ì‚¬ìš©ì: {user_id}, í•­ëª©ìˆ˜: {total_items}")
            
            return CacheStatusResponse(
                total_items=total_items,
                total_size_mb=total_size_mb,
                used_size_mb=used_size_mb,
                available_size_mb=available_size_mb,
                cache_hit_rate=cache_hit_rate,
                oldest_item_date=oldest_date,
                newest_item_date=newest_date,
                data_type_breakdown=data_type_breakdown,
                last_cleanup=datetime.utcnow() - timedelta(hours=6)  # ìƒ˜í”Œ ê°’
            )
            
        except Exception as e:
            logger.error(f"âŒ ìºì‹œ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨ - ì‚¬ìš©ì: {user_id}, ì˜¤ë¥˜: {str(e)}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="ìºì‹œ ìƒíƒœ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            )
    
    async def get_offline_settings(self, user_id: int, organization_id: int) -> OfflineSettingsResponse:
        """
        ì˜¤í”„ë¼ì¸ ì„¤ì •ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
        
        Args:
            user_id: ì‚¬ìš©ì ID
            organization_id: ì¡°ì§ ID
            
        Returns:
            ì˜¤í”„ë¼ì¸ ì„¤ì • ì‘ë‹µ
        """
        try:
            logger.info(f"âš™ï¸ ì˜¤í”„ë¼ì¸ ì„¤ì • ì¡°íšŒ ì‹œì‘ - ì‚¬ìš©ì: {user_id}")
            
            # Redisì—ì„œ ì„¤ì • ì¡°íšŒ
            settings_key = self.settings_key.format(user_id=user_id)
            settings_data = redis_client.get(settings_key)
            
            if settings_data:
                settings = OfflineSettings.parse_raw(settings_data)
            else:
                # ê¸°ë³¸ ì„¤ì • ìƒì„±
                settings = OfflineSettings(
                    user_id=user_id,
                    organization_id=organization_id
                )
                
                # Redisì— ì €ì¥
                redis_client.setex(settings_key, self.cache_ttl, settings.json())
            
            logger.info(f"âœ… ì˜¤í”„ë¼ì¸ ì„¤ì • ì¡°íšŒ ì™„ë£Œ - ì‚¬ìš©ì: {user_id}")
            
            return OfflineSettingsResponse(
                user_id=settings.user_id,
                organization_id=settings.organization_id,
                enabled=settings.enabled,
                auto_sync=settings.auto_sync,
                sync_interval=settings.sync_interval,
                max_cache_size=settings.max_cache_size,
                cache_duration=settings.cache_duration,
                conflict_resolution=settings.conflict_resolution,
                sync_on_startup=settings.sync_on_startup,
                sync_on_network_change=settings.sync_on_network_change,
                background_sync=settings.background_sync,
                compress_data=settings.compress_data,
                encrypt_cache=settings.encrypt_cache,
                created_at=settings.created_at,
                updated_at=settings.updated_at
            )
            
        except Exception as e:
            logger.error(f"âŒ ì˜¤í”„ë¼ì¸ ì„¤ì • ì¡°íšŒ ì‹¤íŒ¨ - ì‚¬ìš©ì: {user_id}, ì˜¤ë¥˜: {str(e)}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="ì˜¤í”„ë¼ì¸ ì„¤ì • ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            )
    
    async def update_offline_settings(
        self,
        user_id: int,
        organization_id: int,
        settings_request: OfflineSettingsRequest
    ) -> OfflineSettingsResponse:
        """
        ì˜¤í”„ë¼ì¸ ì„¤ì •ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        
        Args:
            user_id: ì‚¬ìš©ì ID
            organization_id: ì¡°ì§ ID
            settings_request: ì˜¤í”„ë¼ì¸ ì„¤ì • ìš”ì²­
            
        Returns:
            ì˜¤í”„ë¼ì¸ ì„¤ì • ì‘ë‹µ
        """
        try:
            logger.info(f"âš™ï¸ ì˜¤í”„ë¼ì¸ ì„¤ì • ì—…ë°ì´íŠ¸ ì‹œì‘ - ì‚¬ìš©ì: {user_id}")
            
            # ì„¤ì • ë°ì´í„° ìƒì„±
            settings = OfflineSettings(
                user_id=user_id,
                organization_id=organization_id,
                enabled=settings_request.enabled,
                auto_sync=settings_request.auto_sync,
                sync_interval=settings_request.sync_interval,
                max_cache_size=settings_request.max_cache_size,
                cache_duration=settings_request.cache_duration,
                conflict_resolution=settings_request.conflict_resolution,
                sync_on_startup=settings_request.sync_on_startup,
                sync_on_network_change=settings_request.sync_on_network_change,
                background_sync=settings_request.background_sync,
                compress_data=settings_request.compress_data,
                encrypt_cache=settings_request.encrypt_cache,
                updated_at=datetime.utcnow()
            )
            
            # Redisì— ì €ì¥
            settings_key = self.settings_key.format(user_id=user_id)
            redis_client.setex(settings_key, self.cache_ttl, settings.json())
            
            logger.info(f"âœ… ì˜¤í”„ë¼ì¸ ì„¤ì • ì—…ë°ì´íŠ¸ ì™„ë£Œ - ì‚¬ìš©ì: {user_id}")
            
            return OfflineSettingsResponse(
                user_id=settings.user_id,
                organization_id=settings.organization_id,
                enabled=settings.enabled,
                auto_sync=settings.auto_sync,
                sync_interval=settings.sync_interval,
                max_cache_size=settings.max_cache_size,
                cache_duration=settings.cache_duration,
                conflict_resolution=settings.conflict_resolution,
                sync_on_startup=settings.sync_on_startup,
                sync_on_network_change=settings.sync_on_network_change,
                background_sync=settings.background_sync,
                compress_data=settings.compress_data,
                encrypt_cache=settings.encrypt_cache,
                created_at=settings.created_at,
                updated_at=settings.updated_at
            )
            
        except Exception as e:
            logger.error(f"âŒ ì˜¤í”„ë¼ì¸ ì„¤ì • ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ - ì‚¬ìš©ì: {user_id}, ì˜¤ë¥˜: {str(e)}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="ì˜¤í”„ë¼ì¸ ì„¤ì • ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            )
    
    async def get_conflicts(self, user_id: int) -> ConflictListResponse:
        """
        ì¶©ëŒ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
        
        Args:
            user_id: ì‚¬ìš©ì ID
            
        Returns:
            ì¶©ëŒ ëª©ë¡ ì‘ë‹µ
        """
        try:
            logger.info(f"âš ï¸ ì¶©ëŒ ëª©ë¡ ì¡°íšŒ ì‹œì‘ - ì‚¬ìš©ì: {user_id}")
            
            # Redisì—ì„œ ì¶©ëŒ ëª©ë¡ ì¡°íšŒ
            conflict_key = self.conflict_key.format(user_id=user_id)
            conflict_data = redis_client.get(conflict_key)
            
            conflicts = []
            if conflict_data:
                conflicts_list = json.loads(conflict_data)
                conflicts = [ConflictItem.parse_obj(item) for item in conflicts_list]
            
            # í†µê³„ ê³„ì‚°
            total_count = len(conflicts)
            unresolved_count = len([c for c in conflicts if c.resolution is None])
            
            logger.info(f"âœ… ì¶©ëŒ ëª©ë¡ ì¡°íšŒ ì™„ë£Œ - ì‚¬ìš©ì: {user_id}, ì´ {total_count}ê°œ, ë¯¸í•´ê²° {unresolved_count}ê°œ")
            
            return ConflictListResponse(
                conflicts=conflicts,
                total_count=total_count,
                unresolved_count=unresolved_count,
                last_updated=datetime.utcnow()
            )
            
        except Exception as e:
            logger.error(f"âŒ ì¶©ëŒ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨ - ì‚¬ìš©ì: {user_id}, ì˜¤ë¥˜: {str(e)}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="ì¶©ëŒ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            )
    
    async def resolve_conflict(
        self,
        user_id: int,
        resolution_request: ConflictResolutionRequest
    ) -> ConflictResolutionResponse:
        """
        ì¶©ëŒì„ í•´ê²°í•©ë‹ˆë‹¤.
        
        Args:
            user_id: ì‚¬ìš©ì ID
            resolution_request: ì¶©ëŒ í•´ê²° ìš”ì²­
            
        Returns:
            ì¶©ëŒ í•´ê²° ì‘ë‹µ
        """
        try:
            logger.info(f"âš ï¸ ì¶©ëŒ í•´ê²° ì‹œì‘ - ì‚¬ìš©ì: {user_id}, ì¶©ëŒID: {resolution_request.conflict_id}")
            
            # ì¶©ëŒ ëª©ë¡ ì¡°íšŒ
            conflicts_response = await self.get_conflicts(user_id)
            conflict = None
            
            for c in conflicts_response.conflicts:
                if c.conflict_id == resolution_request.conflict_id:
                    conflict = c
                    break
            
            if not conflict:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail="ì¶©ëŒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )
            
            # í•´ê²° ë°ì´í„° ê²°ì •
            resolved_data = {}
            if resolution_request.resolution == ConflictResolution.SERVER_WINS:
                resolved_data = conflict.server_data
            elif resolution_request.resolution == ConflictResolution.CLIENT_WINS:
                resolved_data = conflict.client_data
            elif resolution_request.resolution == ConflictResolution.MANUAL:
                if not resolution_request.resolved_data:
                    raise HTTPException(
                        status_code=status.HTTP_400_BAD_REQUEST,
                        detail="ìˆ˜ë™ í•´ê²° ì‹œ í•´ê²°ëœ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤."
                    )
                resolved_data = resolution_request.resolved_data
            elif resolution_request.resolution == ConflictResolution.MERGE:
                # ê°„ë‹¨í•œ ë³‘í•© ë¡œì§ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë¡œì§ í•„ìš”)
                resolved_data = {**conflict.server_data, **conflict.client_data}
            
            # ì¶©ëŒ í•´ê²° ì •ë³´ ì—…ë°ì´íŠ¸
            conflict.resolution = resolution_request.resolution
            conflict.resolved_data = resolved_data
            conflict.resolved_at = datetime.utcnow()
            
            # Redisì— ì—…ë°ì´íŠ¸ëœ ì¶©ëŒ ëª©ë¡ ì €ì¥
            updated_conflicts = [c for c in conflicts_response.conflicts if c.conflict_id != conflict.conflict_id]
            updated_conflicts.append(conflict)
            
            conflict_key = self.conflict_key.format(user_id=user_id)
            redis_client.setex(
                conflict_key,
                self.cache_ttl,
                json.dumps([c.dict() for c in updated_conflicts])
            )
            
            logger.info(f"âœ… ì¶©ëŒ í•´ê²° ì™„ë£Œ - ì¶©ëŒID: {resolution_request.conflict_id}")
            
            return ConflictResolutionResponse(
                conflict_id=conflict.conflict_id,
                success=True,
                resolution=conflict.resolution,
                resolved_data=conflict.resolved_data,
                resolved_at=conflict.resolved_at
            )
            
        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"âŒ ì¶©ëŒ í•´ê²° ì‹¤íŒ¨ - ì‚¬ìš©ì: {user_id}, ì˜¤ë¥˜: {str(e)}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="ì¶©ëŒ í•´ê²° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            )
    
    async def get_offline_statistics(self, user_id: int) -> OfflineStatsResponse:
        """
        ì˜¤í”„ë¼ì¸ í†µê³„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
        
        Args:
            user_id: ì‚¬ìš©ì ID
            
        Returns:
            ì˜¤í”„ë¼ì¸ í†µê³„ ì‘ë‹µ
        """
        try:
            logger.info(f"ğŸ“Š ì˜¤í”„ë¼ì¸ í†µê³„ ì¡°íšŒ ì‹œì‘ - ì‚¬ìš©ì: {user_id}")
            
            # ì•¡ì…˜ í í†µê³„
            queue_key = self.action_queue_key.format(user_id=user_id)
            queue_length = redis_client.llen(queue_key)
            
            # ìƒ˜í”Œ í†µê³„ ë°ì´í„° (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°íšŒ)
            total_offline_actions = 150
            pending_actions = queue_length
            completed_actions = 120
            failed_actions = 5
            
            sync_success_rate = (completed_actions / total_offline_actions) * 100 if total_offline_actions > 0 else 0
            
            # ì¶©ëŒ í†µê³„
            conflicts_response = await self.get_conflicts(user_id)
            conflicts_count = conflicts_response.unresolved_count
            
            logger.info(f"âœ… ì˜¤í”„ë¼ì¸ í†µê³„ ì¡°íšŒ ì™„ë£Œ - ì‚¬ìš©ì: {user_id}")
            
            return OfflineStatsResponse(
                total_offline_actions=total_offline_actions,
                pending_actions=pending_actions,
                completed_actions=completed_actions,
                failed_actions=failed_actions,
                sync_success_rate=sync_success_rate,
                average_sync_time=45.5,
                cache_efficiency=88.2,
                data_usage_mb=25.7,
                last_sync_time=datetime.utcnow() - timedelta(minutes=30),
                next_sync_time=datetime.utcnow() + timedelta(minutes=15),
                conflicts_count=conflicts_count,
                last_updated=datetime.utcnow()
            )
            
        except Exception as e:
            logger.error(f"âŒ ì˜¤í”„ë¼ì¸ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨ - ì‚¬ìš©ì: {user_id}, ì˜¤ë¥˜: {str(e)}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="ì˜¤í”„ë¼ì¸ í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            )
    
    async def get_network_status(self) -> NetworkStatusResponse:
        """
        ë„¤íŠ¸ì›Œí¬ ìƒíƒœë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
        
        Returns:
            ë„¤íŠ¸ì›Œí¬ ìƒíƒœ ì‘ë‹µ
        """
        try:
            logger.info(f"ğŸŒ ë„¤íŠ¸ì›Œí¬ ìƒíƒœ ì¡°íšŒ ì‹œì‘")
            
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë„¤íŠ¸ì›Œí¬ ìƒíƒœ í™•ì¸ ë¡œì§ í•„ìš”
            # í˜„ì¬ëŠ” ìƒ˜í”Œ ë°ì´í„° ë°˜í™˜
            
            network_status = NetworkStatusResponse(
                is_online=True,
                connection_type="wifi",
                effective_type="4g",
                downlink=10.5,
                rtt=50,
                save_data=False,
                last_checked=datetime.utcnow()
            )
            
            logger.info(f"âœ… ë„¤íŠ¸ì›Œí¬ ìƒíƒœ ì¡°íšŒ ì™„ë£Œ - ì˜¨ë¼ì¸: {network_status.is_online}")
            return network_status
            
        except Exception as e:
            logger.error(f"âŒ ë„¤íŠ¸ì›Œí¬ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨ - ì˜¤ë¥˜: {str(e)}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="ë„¤íŠ¸ì›Œí¬ ìƒíƒœ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            )
    
    async def clear_cache(self, user_id: int) -> Dict[str, Any]:
        """
        ì‚¬ìš©ì ìºì‹œë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.
        
        Args:
            user_id: ì‚¬ìš©ì ID
            
        Returns:
            ì •ë¦¬ ê²°ê³¼
        """
        try:
            logger.info(f"ğŸ§¹ ìºì‹œ ì •ë¦¬ ì‹œì‘ - ì‚¬ìš©ì: {user_id}")
            
            # ì‚¬ìš©ì ìºì‹œ í‚¤ íŒ¨í„´
            cache_pattern = f"offline:cache:{user_id}:*"
            cache_keys = redis_client.keys(cache_pattern)
            
            deleted_count = 0
            for key in cache_keys:
                redis_client.delete(key)
                deleted_count += 1
            
            logger.info(f"âœ… ìºì‹œ ì •ë¦¬ ì™„ë£Œ - ì‚¬ìš©ì: {user_id}, ì‚­ì œëœ í•­ëª©: {deleted_count}ê°œ")
            
            return {
                "success": True,
                "deleted_items": deleted_count,
                "message": f"{deleted_count}ê°œì˜ ìºì‹œ í•­ëª©ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.",
                "cleared_at": datetime.utcnow()
            }
            
        except Exception as e:
            logger.error(f"âŒ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨ - ì‚¬ìš©ì: {user_id}, ì˜¤ë¥˜: {str(e)}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="ìºì‹œ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            )
    
    async def _execute_sync(
        self,
        task_id: str,
        user_id: int,
        organization_id: int,
        sync_request: SyncRequest
    ):
        """
        ë™ê¸°í™”ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤ (ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…).
        
        Args:
            task_id: ì‘ì—… ID
            user_id: ì‚¬ìš©ì ID
            organization_id: ì¡°ì§ ID
            sync_request: ë™ê¸°í™” ìš”ì²­
        """
        try:
            logger.info(f"ğŸ”„ ë™ê¸°í™” ì‹¤í–‰ ì‹œì‘ - ì‘ì—…ID: {task_id}")
            
            # ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸
            task_key = self.sync_task_key.format(task_id=task_id)
            task_data = redis_client.get(task_key)
            
            if not task_data:
                logger.error(f"ë™ê¸°í™” ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ - ì‘ì—…ID: {task_id}")
                return
            
            sync_task = SyncTask.parse_raw(task_data)
            sync_task.status = SyncStatus.IN_PROGRESS
            sync_task.total_items = 100  # ìƒ˜í”Œ ê°’
            
            # ì§„í–‰ë¥  ì‹œë®¬ë ˆì´ì…˜
            for i in range(0, 101, 10):
                sync_task.progress = i
                sync_task.processed_items = i
                redis_client.setex(task_key, 3600, sync_task.json())
                
                # ì‹¤ì œ ë™ê¸°í™” ì‘ì—… ìˆ˜í–‰
                await asyncio.sleep(1)  # ì‹œë®¬ë ˆì´ì…˜ì„ ìœ„í•œ ì§€ì—°
                
                if i == 50:
                    logger.info(f"ğŸ”„ ë™ê¸°í™” ì§„í–‰ ì¤‘ - ì‘ì—…ID: {task_id}, ì§„í–‰ë¥ : {i}%")
            
            # ì™„ë£Œ ì²˜ë¦¬
            sync_task.status = SyncStatus.COMPLETED
            sync_task.end_time = datetime.utcnow()
            redis_client.setex(task_key, 3600, sync_task.json())
            
            logger.info(f"âœ… ë™ê¸°í™” ì‹¤í–‰ ì™„ë£Œ - ì‘ì—…ID: {task_id}")
            
        except Exception as e:
            logger.error(f"âŒ ë™ê¸°í™” ì‹¤í–‰ ì‹¤íŒ¨ - ì‘ì—…ID: {task_id}, ì˜¤ë¥˜: {str(e)}")
            
            # ì‹¤íŒ¨ ìƒíƒœ ì—…ë°ì´íŠ¸
            try:
                task_key = self.sync_task_key.format(task_id=task_id)
                task_data = redis_client.get(task_key)
                if task_data:
                    sync_task = SyncTask.parse_raw(task_data)
                    sync_task.status = SyncStatus.FAILED
                    sync_task.error_message = str(e)
                    sync_task.end_time = datetime.utcnow()
                    redis_client.setex(task_key, 3600, sync_task.json())
            except Exception as update_error:
                logger.error(f"ë™ê¸°í™” ì‹¤íŒ¨ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ - ì˜¤ë¥˜: {str(update_error)}")
    
    def _compress_data(self, data: str) -> str:
        """ë°ì´í„°ë¥¼ ì••ì¶•í•©ë‹ˆë‹¤."""
        try:
            compressed = gzip.compress(data.encode('utf-8'))
            return base64.b64encode(compressed).decode('utf-8')
        except Exception as e:
            logger.warning(f"ë°ì´í„° ì••ì¶• ì‹¤íŒ¨: {str(e)}")
            return data
    
    def _decompress_data(self, compressed_data: str) -> str:
        """ì••ì¶•ëœ ë°ì´í„°ë¥¼ í•´ì œí•©ë‹ˆë‹¤."""
        try:
            compressed_bytes = base64.b64decode(compressed_data.encode('utf-8'))
            decompressed = gzip.decompress(compressed_bytes)
            return decompressed.decode('utf-8')
        except Exception as e:
            logger.warning(f"ë°ì´í„° ì••ì¶• í•´ì œ ì‹¤íŒ¨: {str(e)}")
            return compressed_data
    
    def _encrypt_data(self, data: str) -> str:
        """ë°ì´í„°ë¥¼ ì•”í˜¸í™”í•©ë‹ˆë‹¤."""
        try:
            encrypted = cipher_suite.encrypt(data.encode('utf-8'))
            return base64.b64encode(encrypted).decode('utf-8')
        except Exception as e:
            logger.warning(f"ë°ì´í„° ì•”í˜¸í™” ì‹¤íŒ¨: {str(e)}")
            return data
    
    def _decrypt_data(self, encrypted_data: str) -> str:
        """ì•”í˜¸í™”ëœ ë°ì´í„°ë¥¼ ë³µí˜¸í™”í•©ë‹ˆë‹¤."""
        try:
            encrypted_bytes = base64.b64decode(encrypted_data.encode('utf-8'))
            decrypted = cipher_suite.decrypt(encrypted_bytes)
            return decrypted.decode('utf-8')
        except Exception as e:
            logger.warning(f"ë°ì´í„° ë³µí˜¸í™” ì‹¤íŒ¨: {str(e)}")
            return encrypted_data